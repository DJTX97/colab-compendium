{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Load utils\n",
        "REGISTRY = (\n",
        "    {\n",
        "        \"id\": \"SOURCE\",\n",
        "        \"name\": \"oobabooga\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"NAME\",\n",
        "        \"name\": \"text-generation-webui\",\n",
        "    },\n",
        ")\n",
        "\n",
        "for opt in REGISTRY:\n",
        "    globals()[opt[\"id\"]] = opt[\"name\"]\n",
        "\n",
        "BASE = f\"https://github.com/{globals()['SOURCE']}/{globals()['NAME']}\"\n",
        "BASE_PATH = f\"/content/{globals()['NAME']}\"\n",
        "NGROK = f\"/content/{globals()['NAME']}/extensions/ngrok/requirements.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwUaatkGgzt2"
      },
      "outputs": [],
      "source": [
        "#@title Prevent disconnections\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCFOzsQSHbjM"
      },
      "outputs": [],
      "source": [
        "#@title Download & install backend\n",
        "!apt-get -y install -qq aria2\n",
        "#!pip install flask-cloudflared\n",
        "!pip install requests-html\n",
        "\n",
        "!git clone $BASE\n",
        "%cd $BASE_PATH\n",
        "!pip install -r requirements.txt\n",
        "!pip install -r $NGROK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p7V3zf_gzt3"
      },
      "outputs": [],
      "source": [
        "#@title Add model\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "from requests_html import HTMLSession\n",
        "\n",
        "OPTIONS = {\n",
        "    1: {\n",
        "        \"id\": \"TheBloke/Echidna-13B-v0.3-GPTQ\", \n",
        "        \"file_list\": []\n",
        "    },\n",
        "    2: {\n",
        "        \"id\": \"Sao10K/Fimbulvetr-11B-v2-GGUF\", \n",
        "        \"file_list\": [\n",
        "            \"https://huggingface.co/Sao10K/Fimbulvetr-11B-v2-GGUF/blob/main/Fimbulvetr-11B-v2.q4_K_S.gguf\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "#@markdown ### Model presets\n",
        "Preset = \"2\" #@param [1,2]\n",
        "\n",
        "OPTION = OPTIONS[int(Preset)]\n",
        "MODEL_ID = OPTION[\"id\"]\n",
        "MODEL_FILES = OPTION[\"file_list\"]\n",
        "\n",
        "#Get model name\n",
        "MODEL_NAME = MODEL_ID.split(MODEL_ID[MODEL_ID.rfind(\"/\")])[-1]\n",
        "\n",
        "#@markdown ### Select model type\n",
        "MODEL_TYPE = \"gguf\" #@param [\"gptq\", \"gguf\"]\n",
        "\n",
        "#@markdown ### model file name based on backend and model type (empty by default)\n",
        "renamed_file = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if MODEL_TYPE == \"gptq\":\n",
        "    renamed_file = \"gptq_model-4bit-128g.safetensors\" \n",
        "\n",
        "if MODEL_TYPE == \"gguf\":\n",
        "    file = list(filter(lambda filename: filename.endswith(\".gguf\"), MODEL_FILES))[0]\n",
        "    renamed_file = file.split(file[file.rfind(\"/\")])[-1]\n",
        "\n",
        "#Get LINKS for GPTQ model\n",
        "if MODEL_TYPE == \"gptq\":\n",
        "    #Target repo\n",
        "    REPO = f\"https://huggingface.co/{MODEL_ID}/tree/main?not-for-all-audiences=true\"\n",
        "\n",
        "    #Get file urls\n",
        "    session = HTMLSession()\n",
        "    result = session.get(REPO)\n",
        "    links = result.html.absolute_links\n",
        "\n",
        "    #Create LINKS list\n",
        "    FILE_TARGETS = (\".json\", \".model\", \".safetensors\", \".py\")\n",
        "    if len(MODEL_FILES) > 0:\n",
        "        LINKS = MODEL_FILES\n",
        "        for i in range(len(LINKS)):\n",
        "            LINKS[i] = LINKS[i].replace(\"/blob/\", \"/resolve/\")\n",
        "    else:\n",
        "        LINKS = []\n",
        "        for link in links:     \n",
        "            if any(target in link for target in FILE_TARGETS) and \"/resolve/\" in link: LINKS.append(link)\n",
        "\n",
        "#Get LINKS for GGUF model (MUST HAVE urls in 'file_list'!)\n",
        "if MODEL_TYPE == \"gguf\":\n",
        "    LINKS = MODEL_FILES\n",
        "    for i in range(len(LINKS)):\n",
        "        LINKS[i] = LINKS[i].replace(\"/blob/\", \"/resolve/\")\n",
        "\n",
        "#Input file for aria2\n",
        "shopping_list = \"down_list.txt\"\n",
        "\n",
        "#Create file for aria2 using LINKS list\n",
        "with open(shopping_list, 'a') as file:\n",
        "    if MODEL_TYPE == \"gguf\":\n",
        "        for url in LINKS:\n",
        "            if \".gguf\" in url: file.write(f\"{url}\\n out={renamed_file}\\n\")\n",
        "    if MODEL_TYPE == \"gptq\":\n",
        "        for url in LINKS:\n",
        "            if \".safetensors\" in url: file.write(f\"{url}\\n out={renamed_file}\\n\")\n",
        "            elif \".model\" in url: file.write(f\"{url}\\n out=tokenizer.model\\n\")\n",
        "            else: file.write(f\"{url}\\n\")\n",
        "\n",
        "#Set models destination for backend\n",
        "models_folder = f\"{BASE_PATH}/models\"\n",
        "\n",
        "#Download all model config files\n",
        "print(f\"Downloading {MODEL_NAME}...\\n\")\n",
        "\n",
        "if MODEL_TYPE == \"gguf\":\n",
        "    !cd $models_folder\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M --summary-interval=5 --input-file=$shopping_list -d $models_folder\n",
        "    !rm $shopping_list\n",
        "\n",
        "if MODEL_TYPE == \"gptq\":\n",
        "    !cd $models_folder && mkdir $MODEL_NAME\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M --summary-interval=5 --input-file=$shopping_list -d $models_folder/$MODEL_NAME\n",
        "    !rm $shopping_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh6BfvYTgzt3"
      },
      "outputs": [],
      "source": [
        "#@title LAUNCH! (Run again in case of cloudflare error)\n",
        "\n",
        "%cd $BASE_PATH\n",
        "\n",
        "provider = \"cloudflare\" #@param [\"ngrok\", \"cloudflare\"]\n",
        "context = \"4096\" #@param {type:\"string\"}\n",
        "gpu_layers = \"60\" #@param {type:\"string\"}\n",
        "\n",
        "CONTEXT = int(context)\n",
        "GPU_LAYERS = int(gpu_layers)\n",
        "\n",
        "if provider == \"ngrok\":\n",
        "    PROVIDER = \"--extension ngrok\"\n",
        "else:\n",
        "    PROVIDER = \"\"\n",
        "\n",
        "if MODEL_TYPE == \"gguf\":\n",
        "    LOADER = \"LLAMACPP\"\n",
        "    MODEL_NAME = renamed_file\n",
        "    !python server.py --share --nowebui --api --public-api $PROVIDER --model $MODEL_NAME --loader $LOADER --max_seq_len $CONTEXT --n_ctx $CONTEXT --n-gpu-layers $GPU_LAYERS\n",
        "\n",
        "if MODEL_TYPE == \"gptq\":\n",
        "    LOADER = \"EXLLAMAV2_HF\"\n",
        "    !python server.py --share --nowebui --api --public-api $PROVIDER --model $MODEL_NAME --loader $LOADER --max_seq_len $CONTEXT --n_ctx $CONTEXT"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
