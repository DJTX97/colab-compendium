{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwUaatkGgzt2"
      },
      "outputs": [],
      "source": [
        "#@title Prevent disconnections\n",
        "%%html\n",
        "<audio src=\"https://oobabooga.github.io/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCFOzsQSHbjM"
      },
      "outputs": [],
      "source": [
        "#@title Main Code\n",
        "!apt-get -y install -qq aria2\n",
        "#!pip install flask-cloudflared\n",
        "!pip install requests-html\n",
        "\n",
        "!git clone https://github.com/oobabooga/text-generation-webui\n",
        "%cd /content/text-generation-webui\n",
        "!pip install -r requirements.txt\n",
        "!pip install -r /content/text-generation-webui/extensions/openai/requirements.txt\n",
        "# !mkdir repositories\n",
        "# %cd /content/text-generation-webui/repositories\n",
        "# !git clone https://github.com/turboderp/exllama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p7V3zf_gzt3"
      },
      "outputs": [],
      "source": [
        "#@title Add model\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "from requests_html import HTMLSession\n",
        "import random\n",
        "import string\n",
        "import secrets\n",
        "\n",
        "OPTIONS = {\n",
        "    1:\"TheBloke/Mythalion-13B-GPTQ\",\n",
        "    2:\"TheBloke/MLewdBoros-L2-13B-GPTQ\",\n",
        "    3:\"TheBloke/MLewdBoros-LRPSGPT-2Char-13B-GPTQ\",\n",
        "    4:\"TheBloke/Xwin-MLewd-13B-v0.2-GPTQ\",\n",
        "    5:\"TheBloke/LLaMA2-13B-TiefighterLR-GPTQ\",\n",
        "    6:\"TheBloke/13B-Thorns-L2-GPTQ\",\n",
        "    7:\"TheBloke/Chronomaid-Storytelling-13B-GPTQ\",\n",
        "    8:\"TheBloke/dolphin-2.0-mistral-7B-GPTQ\",\n",
        "    9:\"TheBloke/Athena-v3-GPTQ\",\n",
        "    10:\"TheBloke/LLaMA2-13B-Psyfighter2-GPTQ\",\n",
        "    11:\"TheBloke/airoboros-l2-13B-2.2.1-GPTQ\",\n",
        "    12:\"TheBloke/Noromaid-13B-v0.3-GPTQ\",\n",
        "    13:\"TheBloke/SOLAR-10.7B-Instruct-v1.0-uncensored-GPTQ\",\n",
        "}\n",
        "\n",
        "#@markdown ### Model presets (Uncheck the box if you use a non-preset model)\n",
        "Preset = \"12\" #@param [1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
        "use_preset = True #@param {type:\"boolean\"}\n",
        "\n",
        "#Black magic\n",
        "CHARSET = string.ascii_letters + string.digits + string.punctuation\n",
        "LENGTH = random.randint(70, 100)\n",
        "Model = r\"\".join(secrets.choice(CHARSET) for _ in range(LENGTH))\n",
        "\n",
        "#ADD NON-PRESET MODEL BETWEEN QUOTES\n",
        "globals()[Model] = \"\" if use_preset == False else OPTIONS[int(Preset)]\n",
        "\n",
        "#@markdown ### For when there are multiple large model files in the same repo. Copy/paste the url of the one you want.\n",
        "large_file_url = \"\" #@param {type:\"string\"}\n",
        "large_file_url = large_file_url.replace(\"/blob/\", \"/resolve/\") if \"/blob/\" in large_file_url else large_file_url\n",
        "\n",
        "#@markdown ### Renamed model file (change based on backend)\n",
        "renamed_file = \"gptq_model-4bit-128g.safetensors\" #@param {type:\"string\"}\n",
        "\n",
        "#Get model name\n",
        "model_name = globals()[Model].split(globals()[Model][globals()[Model].rfind(\"/\")])[-1]\n",
        "\n",
        "#Target repo\n",
        "REPO = globals()[Model] if \"https://\" in globals()[Model] else f\"https://huggingface.co/{globals()[Model]}/tree/main?not-for-all-audiences=true\"\n",
        "\n",
        "#Get file urls\n",
        "session = HTMLSession()\n",
        "result = session.get(REPO)\n",
        "links = result.html.absolute_links\n",
        "\n",
        "#Create LINKS list\n",
        "if len(large_file_url) > 10:\n",
        "    LINKS = [large_file_url]\n",
        "    for link in links:\n",
        "        target = (\".json\" in link, \".model\" in link, \".py\" in link)\n",
        "        if any(target) and \"/resolve/\" in link: LINKS.append(link)\n",
        "else:\n",
        "    LINKS = []\n",
        "    for link in links:\n",
        "        target = (\".json\" in link, \".model\" in link, \".safetensors\" in link, \".pt\" in link, \".py\" in link)\n",
        "        if any(target) and \"/resolve/\" in link: LINKS.append(link)\n",
        "\n",
        "#Name of the file containing urls for aria2\n",
        "shopping_list = \"down_list.txt\"\n",
        "\n",
        "#Create file for aria2 using LINKS list\n",
        "with open(shopping_list, 'a') as file:\n",
        "    for url in LINKS:\n",
        "        target = (\".safetensors\" in url, \".pt\" in url)\n",
        "        if any(target): file.write(f\"{url}\\n out={renamed_file}\\n\")\n",
        "        elif \".model\" in url: file.write(f\"{url}\\n out=tokenizer.model\\n\")\n",
        "        else: file.write(f\"{url}\\n\")\n",
        "\n",
        "#Set models destination for backend\n",
        "models_folder = \"/content/text-generation-webui/models\"\n",
        "\n",
        "#Download all model config files\n",
        "print(f\"Downloading {model_name}...\\n\")\n",
        "!cd $models_folder && mkdir $model_name\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M --summary-interval=5 --input-file=$shopping_list -d $models_folder/$model_name\n",
        "!rm $shopping_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gh6BfvYTgzt3"
      },
      "outputs": [],
      "source": [
        "#@title LAUNCH! (Run again in case of cloudflare error)\n",
        "\n",
        "%cd /content/text-generation-webui\n",
        "!python server.py --nowebui --api --public-api --model $model_name --loader EXLLAMAV2_HF --max_seq_len 4096 --n_ctx 4096"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
